
01 ). What does one mean by the term "Machine Learning" ?

	Machine learning refers to a field of study and technology that enables computers to learn and improve from experience without being explicitly programmed. It involves developing algorithms and models that can analyze data, identify patterns, and make predictions or decisions.
	
	
02 ). Can you think of 4 distinct types of issues where it shines?
	
	Four distinct types of issues where machine learning shines are:
	
	a) Image and speech recognition: Machine learning algorithms can be trained to accurately identify objects, faces, or speech patterns in images and audio data.
	   
	b) Natural language processing: Machine learning is effective in understanding and processing human language, enabling tasks such as language translation, sentiment analysis, and chatbots.
	   
	c) Fraud detection: Machine learning can detect patterns of fraudulent behavior by analyzing large amounts of data, helping to identify potential fraud in financial transactions or cybersecurity.
	   
	d) Recommendation systems: Machine learning algorithms can analyze user preferences and behavior to provide personalized recommendations in various domains, such as e-commerce, music, or content platforms.
	

03 ). What is a labeled training set, and how does it work?
	
	A labeled training set is a dataset where each example or instance is accompanied by a corresponding label or target value. It is used to train a supervised learning model. The labels provide the desired output or prediction for each input, allowing the model to learn the relationship between the input features and the corresponding output. During training, the model adjusts its internal parameters to minimize the difference between its predictions and the true labels in the training set. 
	

04 ). What are the two most important tasks that are supervised?
	
	The two most important tasks that are supervised in machine learning are:
	
	a) Classification: This task involves assigning predefined categories or labels to input instances. For example, classifying emails as spam or not spam based on their content.
	   
	b) Regression: This task involves predicting a continuous value or quantity based on input features. For example, predicting housing prices based on factors like location, size, and number of rooms.

	
05 ). Can you think of four examples of unsupervised tasks?

	Four examples of unsupervised tasks in machine learning are:
	
	a) Clustering: Grouping similar instances together based on their inherent characteristics or patterns without any predefined labels. For example, clustering customer data to identify distinct customer segments.
	   
	b) Dimensionality reduction: Reducing the number of input features while preserving important information. This can be useful for visualizing high-dimensional data or improving the efficiency of other machine learning algorithms.
	   
	c) Anomaly detection: Identifying rare or unusual instances in a dataset that differ significantly from the norm. Anomaly detection can be applied to various domains, such as fraud detection or detecting defective products.
	   
	d) Association rule learning: Discovering interesting relationships or patterns among variables in a large dataset. For example, identifying frequently co-occurring items in market basket analysis.
	
	 
06 ). State the machine learning model that would be best to make a robot walk through various unfamiliar terrains?
	
	The machine learning model that would be best to make a robot walk through various unfamiliar terrains is a reinforcement learning model. Reinforcement learning involves an agent learning to interact with an environment to maximize a reward signal. In this case, the robot would learn through trial and error, receiving feedback or rewards based on its actions, and gradually improving its walking behavior in different terrains. 
	
	
07 ). Which algorithm will you use to divide your customers into different groups?
	
	The algorithm used to divide customers into different groups depends on the specific requirements and characteristics of the data. One commonly used algorithm for customer segmentation is k-means clustering. It groups customers based on similarity in their features or behaviors, dividing them into distinct clusters. Other algorithms like hierarchical clustering or Gaussian mixture models can also be used depending on the complexity of the data and desired outcomes.
	
	
08 ). Will you consider the problem of spam detection to be a supervised or unsupervised learning problem ?
	
	The problem of spam detection is typically considered a supervised learning problem. The algorithm is trained on a labeled dataset where each email is classified as either spam or not spam. The goal is to learn the patterns and characteristics of spam emails to accurately classify new, unseen emails as spam or not spam.


09 ). What is the concept of an online learning system ?
	
	An online learning system is a machine learning system that can learn and adapt continuously in real-time as new data becomes available. It is designed to handle data streams or incremental updates rather than batch processing. In an online learning system, the model is updated dynamically as new data arrives, allowing it to adapt and improve its predictions or decisions over time. This is particularly useful in scenarios where the data distribution or patterns may change frequently or where the model needs to respond quickly to new information.
	
	
10 ). What is out-of-core learning, and how does it differ from core learning ?

	Out-of-core learning is a technique used when the dataset is too large to fit into the computer's memory. In this approach, the data is processed and learned in smaller chunks or batches, reading and writing data to disk as needed. It differs from in-core learning, where the entire dataset can be loaded into memory. Out-of-core learning allows for scalable and efficient training of machine learning models on large datasets that exceed the memory capacity of the system.
	
	 
11 ). What kind of learning algorithm makes predictions using a similarity measure ?
	
	A learning algorithm that makes predictions using a similarity measure is called an instance-based learning algorithm. Instance-based learning methods, such as k-nearest neighbors (k-NN), make predictions by comparing new instances to the labeled instances in the training set and determining their similarity. The prediction is based on the labels of the nearest neighbors in the training set.
	
	
12 ). What's the difference between a model parameter and a hyperparameter in a learning algorithm ?

	In a learning algorithm, a model parameter is a configuration variable that the model learns from the training data. These parameters define the internal state of the model and are adjusted during the learning process. They directly influence the model's predictions. On the other hand, a hyperparameter is a setting or configuration that is external to the model and needs to be set before training. Hyperparameters control the behavior of the learning algorithm itself, such as the learning rate or the number of hidden layers in a neural network. They are not learned from the data but rather set by the user or chosen through optimization techniques.
	
	
13 ). What are the criteria that model-based learning algorithms look for ? What is the most popular method they use to achieve success? What method do they use to make predictions ?
 
	Model-based learning algorithms look for a model that best fits the training data. They aim to capture the underlying patterns or relationships in the data to make accurate predictions or decisions. The most popular method used by model-based learning algorithms is optimization, where the algorithm finds the best model parameters that minimize a certain loss or error function. Once the model is trained, it can make predictions by applying the learned patterns or relationships to new, unseen data. 
	
	
14 ). Can you name four of the most important Machine Learning challenges ?
	
	Four important machine learning challenges are:
	
	a) Data quality and preprocessing: Machine learning heavily relies on the quality and relevance of the input data. Preprocessing tasks, such as data cleaning, handling missing values, and feature engineering, are crucial to ensure accurate and meaningful results.
	
	b) Overfitting and underfitting: Overfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data. Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data. Finding the right balance and avoiding these issues is a challenge in machine learning.
	
	c) Interpretability and explain-ability: Some machine learning models, such as deep neural networks, are often considered black boxes, making it challenging to understand how they arrive at their predictions. Interpretable and explainable models are important in domains where transparency and accountability are crucial.
	
	d) Scalability and efficiency: As datasets and models grow in size and complexity, scalability and efficiency become significant challenges. Training and deploying large-scale models efficiently require careful optimization and consideration of computational resources.
	
	
15 ). What happens if the model performs well on the training data but fails to generalize the results to
new situations? Can you think of three different options ?
	
	If a model performs well on the training data but fails to generalize to new situations, three different options can be considered:
	
	a) Collect more diverse and representative data: The model may be overfitting to the training data due to insufficient or biased samples. By gathering more diverse data, the model can learn more generalizable patterns and improve its performance on new situations.
	
	b) Adjust model complexity or regularization: Overly complex models can lead to overfitting. By reducing the complexity or applying regularization techniques, such as L1 or L2 regularization, the model can generalize better and avoid fitting the noise in the training data.
	
	c) Try different algorithms or architectures: Sometimes, the choice of algorithm or model architecture may not be suitable for the given problem. Exploring alternative algorithms or architectures that are better suited to the data and problem domain can help improve generalization performance. 
	
	
16 ). What exactly is a test set, and why would you need one ?
	
	A test set is a separate portion of the labeled dataset that is used to evaluate the performance of a trained machine learning model. It consists of examples that the model has not seen during the training process. The test set allows assessing how well the model generalizes to new, unseen data by measuring its accuracy or other performance metrics. It provides an unbiased evaluation of the model's performance and helps determine if it is ready for deployment. 
	
	
17 ). What is a validation set's purpose ?
	
	The purpose of a validation set is to fine-tune the hyperparameters and evaluate different model variations during the model development phase. It is a subset of the labeled training data that is held out and not used for model training. Instead, it is used to assess the performance of different hyperparameter settings or model configurations. The validation set helps in making informed decisions regarding hyperparameter tuning, model selection, and avoiding overfitting to the training data.
	
	
18 ). What precisely is the train-dev kit, when will you need it, how do you put it to use ?
	
	The train-dev kit, also known as a development set or hold-out set, is a portion of the labeled training data that is set aside and not used during the model development process. It is used to simulate real-world performance and identify potential issues like dataset shift or model degradation. The train-dev kit helps in diagnosing problems and making improvements to the model before final evaluation on the test set. It serves as an intermediate step between training and evaluating the model's performance. 
	
	
19 ). What could go wrong if you use the test set to tune hyperparameters ?
	
	If the test set is used to tune hyperparameters, it can lead to overfitting to the test set and an overly optimistic evaluation of the model's performance. This can result in a model that performs well on the test set but fails to generalize to new data. To avoid this issue, it is essential to separate the test set from the process of hyperparameter tuning. Hyperparameters should be tuned using a separate validation set or through techniques like cross-validation. This ensures that the test set remains independent for unbiased evaluation of the final model's performance.  
	
---
